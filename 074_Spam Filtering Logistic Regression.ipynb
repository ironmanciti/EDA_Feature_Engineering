{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a814ee10",
   "metadata": {},
   "source": [
    "# Spam Filtering Logistic Regression\n",
    "\n",
    "- https://www.kaggle.com/uciml/sms-spam-collection-dataset  \n",
    "\n",
    "- SMS 스팸 컬렉션은 태그가 지정된 SMS 메시지 집합입니다. 여기에는 5,574 개 메시지 중 영어로 된 SMS 메시지 한 세트가 포함되어 있으며 햄 (합법적) 또는 스팸으로 태그가 지정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21fdc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sms = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\", \n",
    "                  usecols=[0, 1], skiprows=1, names=[\"label\", \"message\"])\n",
    "\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-lemon",
   "metadata": {},
   "source": [
    "- label value 는 spam, ham 두가지  \n",
    "- ham 은 0, spam 은 1 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f12207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4a885",
   "metadata": {},
   "source": [
    "- label column을 numeric으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39aa3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.label = sms.label.map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee33fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a6682",
   "metadata": {},
   "source": [
    "### Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df2f762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4457,), (1115,), (4457,), (1115,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sms.message, sms.label, test_size=0.2)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2e943",
   "metadata": {},
   "source": [
    "## Text Data 전처리\n",
    "\n",
    "### Bag of Word 문서 생성\n",
    "\n",
    "Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다. 텍스트 문서에 있는 단어들을 가방에다가 전부 넣습니다. 그러고나서 이 가방을 흔들어 단어들을 섞습니다. 만약, 해당 문서 내에서 특정 단어가 N번 등장했다면, 이 가방에는 그 특정 단어가 N개 있게됩니다. 또한 가방을 흔들어서 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않습니다.\n",
    "\n",
    "- BoW를 만드는 과정.  \n",
    "(1) 각 단어에 고유한 정수 인덱스를 부여합니다.  \n",
    "(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.\n",
    "\n",
    "\n",
    "- 문장을 token화 하고 각 문장에 token 이 몇 번 등장하는지 count  \n",
    "\n",
    "- 각 token 을 feature 화하여 feature(단어) 출현 횟수에 따라 spam 여부 분류\n",
    "\n",
    "- CountVectorizer\n",
    "    - min_df : vocabulary 에 포함할 최소 발생 빈도\n",
    "    - ngram_range : (1, 1) - unigram only, (1, 2) - unigram + bigram\n",
    "    - max_features : top max_features 만으로 vocabulary 구성\n",
    "    - token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b : unocode 영수자 2 글자 이상만 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf04ec",
   "metadata": {},
   "source": [
    "### Token화된 Train document matrix  생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "separated-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "couvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "young-pearl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4457x7701 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 58873 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tokenized = couvec.fit_transform(X_train)\n",
    "X_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "secret-token",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 수: 4457\n",
      "단어수: 7700\n"
     ]
    }
   ],
   "source": [
    "print(f\"document 수: {X_train_tokenized.shape[0]}\")\n",
    "print(f\"단어수: {X_train_tokenized.shape[1]-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "removable-newcastle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>...</th>\n",
       "      <th>ì¼1</th>\n",
       "      <th>ìä</th>\n",
       "      <th>ìï</th>\n",
       "      <th>û_</th>\n",
       "      <th>û_thanks</th>\n",
       "      <th>ûªve</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharry</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4457 rows × 7701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  000pes  008704050406  0121  01223585236  01223585334  \\\n",
       "0      0    0       0             0     0            0            0   \n",
       "1      0    0       0             0     0            0            0   \n",
       "2      0    0       0             0     0            0            0   \n",
       "3      0    0       0             0     0            0            0   \n",
       "4      0    0       0             0     0            0            0   \n",
       "...   ..  ...     ...           ...   ...          ...          ...   \n",
       "4452   0    0       0             0     0            0            0   \n",
       "4453   0    0       0             0     0            0            0   \n",
       "4454   0    0       0             0     0            0            0   \n",
       "4455   0    0       0             0     0            0            0   \n",
       "4456   0    0       0             0     0            0            0   \n",
       "\n",
       "      0125698789  02  0207  ...  ì¼1  ìä  ìï  û_  û_thanks  ûªve  ûï  ûïharry  \\\n",
       "0              0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "1              0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "2              0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "3              0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "4              0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "...          ...  ..   ...  ...  ...  ..  ..  ..       ...   ...  ..      ...   \n",
       "4452           0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "4453           0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "4454           0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "4455           0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "4456           0   0     0  ...    0   0   0   0         0     0   0        0   \n",
       "\n",
       "      ûò  ûówell  \n",
       "0      0       0  \n",
       "1      0       0  \n",
       "2      0       0  \n",
       "3      0       0  \n",
       "4      0       0  \n",
       "...   ..     ...  \n",
       "4452   0       0  \n",
       "4453   0       0  \n",
       "4454   0       0  \n",
       "4455   0       0  \n",
       "4456   0       0  \n",
       "\n",
       "[4457 rows x 7701 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bow = pd.DataFrame(X_train_tokenized.toarray(), columns = couvec.get_feature_names_out())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a83a5720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lasting', 'late', 'lately', 'latelyxxx', 'later', 'latest', 'latests',\n",
       "       'latr', 'laugh', 'laughing', 'laughs', 'laundry', 'laurie', 'lautech',\n",
       "       'law', 'laxinorficated', 'lay', 'lays', 'lazy', 'lccltd', 'ld', 'ldew',\n",
       "       'ldn', 'ldnw15h', 'le', 'lead', 'leadership', 'leading', 'leaf',\n",
       "       'league', 'leanne', 'learn', 'learned', 'least', 'least5times', 'leave',\n",
       "       'leaves', 'leaving', 'lect', 'lecture', 'lecturer', 'left', 'leftovers',\n",
       "       'leg', 'legal', 'legs', 'leh', 'lei', 'lekdog', 'lemme', 'length',\n",
       "       'lennon', 'leona', 'leonardo', 'les', 'less', 'lesser', 'lesson',\n",
       "       'lessons', 'let', 'lets', 'letter', 'letters', 'level', 'lf56', 'li',\n",
       "       'liao', 'lib', 'library', 'lick', 'licks', 'lido', 'lie', 'lies',\n",
       "       'life', 'lifebook', 'lifeis', 'lifetime', 'lifpartnr', 'lift', 'lifted',\n",
       "       'light', 'lighters', 'lightly', 'lik', 'like', 'liked', 'likely',\n",
       "       'likes', 'likeyour', 'liking', 'lil', 'lily', 'limit', 'limits',\n",
       "       'limping', 'lindsay', 'line', 'linear', 'lined'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.columns[4000:4100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490e815",
   "metadata": {},
   "source": [
    "- test data 는 train data 에서 fit 한 count vectorizer 를 이용하여 transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f0c662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1115x7701 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13998 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tokenized = couvec.transform(X_test)\n",
    "X_test_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec206c8",
   "metadata": {},
   "source": [
    "### 이진 분류기 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a6b087a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier = LogisticRegression(random_state=0)\n",
    "lr_classifier.fit(X_train_tokenized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba5b38",
   "metadata": {},
   "source": [
    "### predict\n",
    "\n",
    "- predict() - 예측된 class 를 threshold 0.5 기준으로 반환\n",
    "- predict_proba() - class 별 probability 를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b73fdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "\n",
      "Test set 의 true counts =  160\n",
      "모델이 예측한 predicted true counts =  143\n",
      "accuracy = 0.98\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_classifier.predict(X_test_tokenized)\n",
    "\n",
    "print(y_pred)\n",
    "print()\n",
    "print(\"Test set 의 true counts = \", sum(y_test))\n",
    "print(\"모델이 예측한 predicted true counts = \", sum(y_pred))\n",
    "print(\"accuracy = {:.2f}\".format(sum(y_pred == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c535148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
